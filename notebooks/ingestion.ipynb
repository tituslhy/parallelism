{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c69430d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49c5603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ade7c2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/processors.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/processors.py\n",
    "import asyncio\n",
    "import os\n",
    "import logging\n",
    "import uuid\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from typing import Any\n",
    "\n",
    "import ollama\n",
    "from langchain_text_splitter import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import AsyncQdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "from qdrant_client.http import models\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def process_document_chunks(document_data: dict) -> dict:\n",
    "    \"\"\"Process a single document into chunks.\"\"\"\n",
    "    doc_id = document_data['id']\n",
    "    content = document_data['content']\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 1000,\n",
    "        chunk_overlap = 200,\n",
    "        length_function = len,\n",
    "        separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    raw_chunks = text_splitter.split_text(content)\n",
    "    chunks = []\n",
    "    for i, chunk_text in enumerate(raw_chunks):\n",
    "        if len(chunk_text.strip()) < 50: #we skip tiny chunks here\n",
    "            continue\n",
    "        chunk = {\n",
    "            \"id\": f\"{doc_id}_chunk_{i}\",\n",
    "            \"text\": chunk_text.strip(),\n",
    "            \"word_count\": len(chunk_text.split()),\n",
    "            \"chunk_index\": i,\n",
    "            \"document_id\": doc_id\n",
    "        }\n",
    "        chunks.append(chunk)\n",
    "    return {\n",
    "        \"document_id\": doc_id,\n",
    "        \"chunks\": chunks,\n",
    "        \"total_chunks\": len(chunks),\n",
    "        \"processed_by_pid\": os.getpid()\n",
    "    }\n",
    "\n",
    "class OllamaEmbeddingClient:\n",
    "    def __init__(self, model: str = \"nomic-embed-text\", embedding_size: int = 768):\n",
    "        self.model = model\n",
    "        self.embedding_size = embedding_size\n",
    "        self._client = ollama.AsyncClient()\n",
    "\n",
    "    async def generate_embedding(self, text: str) -> list[float]:\n",
    "        \"\"\"Generate embedding for a single text using the specified model.\"\"\"\n",
    "        response = await self._client.embeddings(model=self.model, prompt=text)\n",
    "        return response['embedding']\n",
    "\n",
    "    async def generate_embeddings_batch(self, texts: list[str]) -> list[list[float]]:\n",
    "        tasks = [self.generate_embedding(text, self.model) for text in tqdm(texts)]\n",
    "\n",
    "        ## Control concurrency to avoid overwhelming Ollama\n",
    "        batch_size = 5\n",
    "        embeddings = []\n",
    "\n",
    "        for i in range(0, len(tasks), batch_size):\n",
    "            batch = tasks[i : i + batch_size]\n",
    "            batch_results = await asyncio.gather(*batch, return_exceptions=True)\n",
    "\n",
    "            for result in batch_results:\n",
    "                if isinstance(result, Exception):\n",
    "                    raise \n",
    "                else:\n",
    "                    embeddings.append(result)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    @property\n",
    "    def client(self):\n",
    "        return self._client\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        qdrant_url: str = \"http://localhost:6333\",\n",
    "        collection_name: str = \"documents\",\n",
    "    ):\n",
    "        self.qdrant_client = AsyncQdrantClient(url=qdrant_url)\n",
    "        self.collection_name = collection_name\n",
    "        self.ollama_client = OllamaEmbeddingClient()\n",
    "\n",
    "        ## Instantiate queues\n",
    "        self.upload_queue = asyncio.Queue(maxsize=100)\n",
    "        self.processing_queue = asyncio.Queue(maxsize=50)\n",
    "        self.embedding_queue = asyncio.Queue(maxsize=20)\n",
    "        self.storage_queue = asyncio.Queue(maxsize=10)\n",
    "\n",
    "        ## Locks for shared resources\n",
    "        self.stats_lock = asyncio.Lock()\n",
    "        self.qdrant_lock = asyncio.Lock()\n",
    "\n",
    "        self.stats = {\n",
    "            \"total_uploads\": 0,\n",
    "            \"documents_processed\": 0,\n",
    "            \"embeddings_generated\": 0,\n",
    "            \"documents_stored\": 0,\n",
    "            \"failed\": 0\n",
    "        }\n",
    "        self.document_status = {}\n",
    "        self.workers_started = False\n",
    "\n",
    "    async def initialize_qdrant(self):\n",
    "        \"\"\"Initialize a Qdrant collection if it doesn't exist.\"\"\"\n",
    "\n",
    "        try:\n",
    "            collections = await self.qdrant_client.get_collections()\n",
    "            collection_exists = any(c.name == self.collection_name for c in collections.collections)\n",
    "\n",
    "            if not collection_exists:\n",
    "                logger.info(f\"🔧 Creating Qdrant collection: {self.collection_name}\")\n",
    "                await self.qdrant_client.create_collection(\n",
    "                    collection_name=self.collection_name,\n",
    "                    vectors_config = VectorParams(\n",
    "                        size = self.ollama_client.embedding_size,\n",
    "                        distance = Distance.COSINE\n",
    "                    )\n",
    "                )\n",
    "                logger.info(f\"✅ Qdrant collection created: {self.collection_name}\")\n",
    "            else:\n",
    "                logger.info(f\"✅ Qdrant collection already exists: {self.collection_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error initializing Qdrant collection: {e}\")\n",
    "            raise\n",
    "\n",
    "    async def start_workers(self):\n",
    "        \"\"\"Start all background worker tasks.\"\"\"\n",
    "\n",
    "        if self.workers_started:\n",
    "            return\n",
    "\n",
    "        # Initialize Qdrant first\n",
    "        await self.initialize_qdrant()\n",
    "\n",
    "        self.worker_tasks = [\n",
    "            # Document processors (CPU intensive)\n",
    "            asyncio.create_task(self.document_processor_worker(1)),\n",
    "            asyncio.create_task(self.document_processor_worker(2)),\n",
    "\n",
    "            # Embedding generators (I/O with Ollama, but rate limited)\n",
    "            asyncio.create_task(self.embedding_worker(1)),\n",
    "            asyncio.create_task(self.embedding_worker(2)),\n",
    "\n",
    "            # Qdrant storage workers (I/O intensive)\n",
    "            asyncio.create_task(self.qdrant_storage_worker(1)),\n",
    "            asyncio.create_task(self.qdrant_storage_worker(2)),\n",
    "\n",
    "            # Stats reporter\n",
    "            asyncio.create_task(self.stats_worker())\n",
    "        ]\n",
    "\n",
    "        self.workers_started = True\n",
    "        logger.info(\"🚀 All workers started with Qdrant + Ollama!\")\n",
    "\n",
    "    async def add_document_to_queue(self, user_id: str, filename: str, content: str) -> str:\n",
    "        \"\"\"Add document to the processing queue\"\"\"\n",
    "\n",
    "        doc_id = str(uuid.uuid4())\n",
    "        document_item = {\n",
    "            \"id\": doc_id,\n",
    "            \"user_id\": user_id,\n",
    "            \"filename\": filename,\n",
    "            \"content\": content,\n",
    "            \"upload_time\": datetime.isoformat(),\n",
    "            'status': 'queued'\n",
    "        } \n",
    "        await self.upload_queue.put(document_item)\n",
    "        self.document_status[doc_id] = \"queued\"\n",
    "\n",
    "        ## Use log to prevent race conditions for logging and updating class statistics\n",
    "        async with self.stats_lock:\n",
    "            self.stats['total_uploads'] += 1\n",
    "            logger.info(f\"📤 Document {filename} queued for processing\")\n",
    "\n",
    "        return doc_id\n",
    "\n",
    "    async def document_processor_worker(self, worker_id: int):\n",
    "        \"\"\"Processes documents into chunks using multiprocessing.\"\"\"\n",
    "\n",
    "        logger.info(f\"🔄 Document processor {worker_id} started\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                # Get first item in the queue\n",
    "                doc_item = await self.upload_queue.get()\n",
    "                doc_id = doc_item['id']\n",
    "                logger.info(f\"🔄 Processor {worker_id}: Chunking {doc_item['filename']}\")\n",
    "                self.document_status[doc_id] = \"processing\"\n",
    "\n",
    "                # CPU-intensive chunking in separate process\n",
    "                loop = asyncio.get_running_loop()\n",
    "                with ProcessPoolExecutor() as executor:\n",
    "                    processed_result = await loop.run_in_executor(\n",
    "                        executor, process_document_chunks, doc_item\n",
    "                    )\n",
    "\n",
    "                # Add to embedding queue\n",
    "                embedding_item = {\n",
    "                    **doc_item,\n",
    "                    'chunks': processed_result['chunks'],\n",
    "                    'processing_completed_at': datetime.isoformat(),\n",
    "                }\n",
    "\n",
    "                await self.processing_queue.put(embedding_item)\n",
    "\n",
    "                async with self.stats_lock:\n",
    "                    self.stats['document_processed'] += 1\n",
    "\n",
    "                self.document_status[doc_id] = 'chunks_created'\n",
    "                self.upload_queue.task_done()\n",
    "\n",
    "                logger.info(f\"✅ Processor {worker_id}: Created {len(processed_result['chunks'])} chunks\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Processor {worker_id} error: {e}\")\n",
    "                if 'doc_id' in locals():\n",
    "                    self.document_status[doc_id] = 'failed'\n",
    "                async with self.stats_lock:\n",
    "                    self.stats['failed'] +=1\n",
    "                self.upload_queue.task_done()\n",
    "\n",
    "    async def embedding_worker(self, worker_id: int):\n",
    "        \"\"\"Generates embeddings using Ollama\"\"\"\n",
    "\n",
    "        logger.info(f\"🧠 Embedding worker {worker_id} started\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                doc_item = await self.processing_queue.get()\n",
    "                doc_id = doc_item['id']\n",
    "\n",
    "                logger.info(f\"🧠 Embedder {worker_id}: Generating embeddings for {doc_item['filename']}\")\n",
    "                self.document_status[doc_id] = \"generating_embeddings\"\n",
    "\n",
    "                #Extract texts from chunks\n",
    "                chunk_texts = [chunk['text'] for chunk in doc_item['chunks']]\n",
    "\n",
    "                #Generate embeddings using Ollama (I/O bound but rate limited)\n",
    "                embeddings = await self.ollama_client.generate_embeddings_batch(chunk_texts)\n",
    "\n",
    "                #combine chunks with embeddings\n",
    "                enriched_chunks = []\n",
    "                for chunk, embedding in zip(doc_item['chunks'], embeddings):\n",
    "                    enriched_chunks.append({\n",
    "                        **chunk,\n",
    "                        'embedding': embedding,\n",
    "                        'embedded_at': datetime.isoformat()\n",
    "                    })\n",
    "\n",
    "                # Add to storage queue\n",
    "                storage_item = {\n",
    "                    **doc_item,\n",
    "                    'enriched_chunks': enriched_chunks,\n",
    "                    'embeddings_completed_at': datetime.isoformat(),\n",
    "                }\n",
    "\n",
    "                await self.embedding_queue.put(storage_item)\n",
    "\n",
    "                async with self.stats_lock:\n",
    "                    self.stats['embeddings_generated'] += len(embeddings)\n",
    "\n",
    "                self.document_status[doc_id] = 'embeddings_ready'\n",
    "                self.processing_queue.task_done()\n",
    "\n",
    "                logger.info(f\"✅ Embedder {worker_id}: Generated {len(embeddings)} embeddings\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Embedder {worker_id} error: {e}\")\n",
    "                if 'doc_id' in locals():\n",
    "                    self.document_status[doc_id] = 'failed'\n",
    "                async with self.stats_lock:\n",
    "                    self.stats['failed'] +=1\n",
    "                self.processing_queue.task_done()\n",
    "\n",
    "    async def qdrant_storage_worker(self, worker_id: int):\n",
    "        \"\"\"Stores embeddings in Qdrant vector database\"\"\"\n",
    "\n",
    "        logger.info(f\"💾 Qdrant storage worker {worker_id} started\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                doc_item = await self.embedding_queue.get()\n",
    "                doc_id = doc_item['id']\n",
    "\n",
    "                logger.info(f\"💾 Storage {worker_id}: Storing embeddings for {doc_item['filename']}\")\n",
    "                self.document_status[doc_id] = \"storing_embeddings\"\n",
    "\n",
    "                # Prepare points for Qdrant\n",
    "                points = []\n",
    "                for chunk in doc_item['enriched_chunks']:\n",
    "                    point = PointStruct(\n",
    "                        id=chunk['id'],\n",
    "                        vector=chunk['embedding'],\n",
    "                        payload={\n",
    "                            'document_id': doc_id,\n",
    "                            'user_id': doc_item['user_id'],\n",
    "                            'filename': doc_item['filename'],\n",
    "                            'chunk_index': chunk['chunk_index'],\n",
    "                            'text': chunk['text'],\n",
    "                            'word_count': chunk['word_count'],\n",
    "                            'uploaded_at': doc_item['uploaded_at'],\n",
    "                            'embedded_at': chunk['embedded_at']\n",
    "                        }\n",
    "                    )\n",
    "                    points.append(point)\n",
    "\n",
    "                # Batch upsert to Qdrant\n",
    "                async with self.qdrant_lock:\n",
    "                    await self.qdrant_client.upsert(\n",
    "                        collection_name=self.collection_name,\n",
    "                        points=points\n",
    "                    )\n",
    "\n",
    "                async with self.stats_lock:\n",
    "                    self.stats['documents_stored'] += 1\n",
    "\n",
    "                self.document_status[doc_id] = 'stored'\n",
    "                self.embedding_queue.task_done()\n",
    "\n",
    "                logger.info(f\"✅ Storage {worker_id}: {doc_item['filename']} completed!\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Storage {worker_id} error: {e}\")\n",
    "                if 'doc_id' in locals():\n",
    "                    self.document_status[doc_id] = 'failed'\n",
    "                async with self.stats_lock:\n",
    "                    self.stats['failed'] +=1\n",
    "                self.embedding_queue.task_done()\n",
    "\n",
    "    async def stats_worker(self):\n",
    "        \"\"\"Background stats reporter\"\"\"\n",
    "\n",
    "        while True:\n",
    "            await asyncio.sleep(15)  # Report every 15 seconds\n",
    "            async with self.stats_lock:\n",
    "                stats = self.stats.copy()\n",
    "\n",
    "            # Get Qdrant collection info\n",
    "            try:\n",
    "                collection_info = await self.qdrant_client.get_collection(self.collection_name)\n",
    "                vector_count = collection_info.points_count\n",
    "            except:\n",
    "                vector_count = \"unknown\"\n",
    "\n",
    "            logger.info(f\"📊 STATS: {stats} | Qdrant vectors: {vector_count}\")\n",
    "            logger.info(\n",
    "                f\"📋 Queues - Upload: {self.upload_queue.qsize()}, \"\n",
    "                f\"Processing: {self.processing_queue.qsize()}, \"\n",
    "                f\"Embedding: {self.embedding_queue.qsize()}\"\n",
    "            )\n",
    "\n",
    "    async def semantic_search(self, query: str, limit: int = 5) -> list[str]:\n",
    "        \"\"\"Search for similar documents using Qdrant\"\"\"\n",
    "\n",
    "        try:\n",
    "            query_embedding = await self.ollama_client.generate_embedding(query)\n",
    "\n",
    "            search_result = await self.qdrant_client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_embedding,\n",
    "                limit=limit,\n",
    "                with_payload=True\n",
    "            )\n",
    "\n",
    "            # Format results\n",
    "\n",
    "            results = []\n",
    "            for result in search_result:\n",
    "                results.append({\n",
    "                    'chunk_id': result.id,\n",
    "                    'score': result.score,\n",
    "                    'text': result.payload['text'],\n",
    "                    'document_id': result.payload['document_id'],\n",
    "                    'filename': result.payload['filename'],\n",
    "                    'user_id': result.payload['user_id']\n",
    "                })\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Semantic search error: {e}\")\n",
    "            return []\n",
    "\n",
    "    async def get_document_status(self, doc_id: str) -> dict:\n",
    "        \"\"\"Get current processing status of a document\"\"\"\n",
    "\n",
    "        status = self.document_status.get(doc_id, 'not_found')\n",
    "        async with self.stats_lock:\n",
    "            current_stats = self.stats.copy()\n",
    "\n",
    "        return {\n",
    "            \"document_id\": doc_id,\n",
    "            \"status\": status,\n",
    "            \"stats\": current_stats\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d274cc29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parallelism",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
