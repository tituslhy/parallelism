{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96537f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef0d64e",
   "metadata": {},
   "source": [
    "Import this library to nest asynchronous operations within Jupyter notebooks\n",
    "> Jupyter notebooks are asynchronous by nature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27a34814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715b59cc",
   "metadata": {},
   "source": [
    "## 1. The difference between a synchronous and asynchronous function\n",
    "Synchronous functions are \"thread-blocking\". This means that no other tasks can commence unless this task is completed! \n",
    "\n",
    "In contrast, asynchronous functions are non-thread blocking. We allow for other tasks to be run concurrently with the current running tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f2b128",
   "metadata": {},
   "source": [
    "Let's mock up a list of \"documents\" for processing to simulate an I/O intensive task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a20996e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [f\"doc {i}\" for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762aae34",
   "metadata": {},
   "source": [
    "### The \"normal\" synchronous method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14dae8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8769866a33ce47ecad55884a7a879b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 0 processed.\n",
      "doc 1 processed.\n",
      "doc 2 processed.\n",
      "doc 3 processed.\n",
      "doc 4 processed.\n",
      "Total time taken: 25.035959005355835 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def process_document(document_name: str) -> str:\n",
    "    time.sleep(5) # simulating a blocking task    \n",
    "    return f\"{document_name} processed.\"\n",
    "\n",
    "for doc in tqdm(docs):\n",
    "    print(process_document(doc))\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c35a656",
   "metadata": {},
   "source": [
    "### The asynchronous approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ccb83",
   "metadata": {},
   "source": [
    "#### Step 1: Define the method to be an asynchronous function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12850c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def aprocess_document(document_name: str) -> str:\n",
    "    await asyncio.sleep(5) # simulating a blocking task    \n",
    "    return f\"{document_name} processed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38f0af5",
   "metadata": {},
   "source": [
    "#### Step 2: Invoking the asynchronous function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507fc0f9",
   "metadata": {},
   "source": [
    "Do not do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1178dc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3c5c9ee46641898c62e2ecbd00eb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 25.01995015144348 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for doc in tqdm(docs):\n",
    "    result = await aprocess_document(doc)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e644f95",
   "metadata": {},
   "source": [
    "The above code block still runs the functions sequentially (not concurrently) which is why there's no speed improvement - just that the function invocations are not thread blocking, which means that other funtions can run at the same time.\n",
    "\n",
    "Instead we invoke this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59c4dc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 5.002790212631226 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "results = await asyncio.gather(\n",
    "    *[aprocess_document(doc) for doc in docs]\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56be535d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc 0 processed.',\n",
       " 'doc 1 processed.',\n",
       " 'doc 2 processed.',\n",
       " 'doc 3 processed.',\n",
       " 'doc 4 processed.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4239936",
   "metadata": {},
   "source": [
    "Wow what a bump! But why!\n",
    "\n",
    "asyncio *starts* all the tasks at the same time. Then gathers the results once the tasks completes. Since all tasks are running concurrently, the time taken to finish is faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001d2132",
   "metadata": {},
   "source": [
    "### So why don't we make everything `async`?\n",
    "Let's just always not block the thread?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65207c49",
   "metadata": {},
   "source": [
    "#### 1. Easier to debug tracebacks of synchronous functions\n",
    "\n",
    "Consider the function below. We expect the function to always blow up when n = 2 regardless of whether the call is synchronous or asynchronous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995e138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 0\n",
      "doc 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Bad doc 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdoc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[32m5\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mfetch_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mfetch_doc\u001b[39m\u001b[34m(n)\u001b[39m\n\u001b[32m      2\u001b[39m time.sleep(\u001b[32m0.1\u001b[39m * n) \u001b[38;5;66;03m# simulate I/O\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n == \u001b[32m2\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBad doc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdoc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Bad doc 2"
     ]
    }
   ],
   "source": [
    "def fetch_doc(n):\n",
    "    time.sleep(0.1 * n) # simulate I/O\n",
    "    if n == 2:\n",
    "        raise ValueError(f\"Bad doc {n}\")\n",
    "    return f\"doc {n}\"\n",
    "\n",
    "for i in range (5):\n",
    "    print(fetch_doc(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a726fbb2",
   "metadata": {},
   "source": [
    "This traceback is very simple to debug. Now consider the next one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5acbbef6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Bad doc 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdoc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m tasks = [asyncio.create_task(afetch_doc(i)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m5\u001b[39m)]\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/py313/lib/python3.13/asyncio/tasks.py:375\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    377\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    378\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/py313/lib/python3.13/asyncio/tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mafetch_doc\u001b[39m\u001b[34m(n)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(\u001b[32m0.1\u001b[39m * n) \u001b[38;5;66;03m# simulate I/O\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n == \u001b[32m2\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBad doc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdoc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Bad doc 2"
     ]
    }
   ],
   "source": [
    "async def afetch_doc(n):\n",
    "    await asyncio.sleep(0.1 * n) # simulate I/O\n",
    "    if n == 2:\n",
    "        raise ValueError(f\"Bad doc {n}\")\n",
    "    return f\"doc {n}\"\n",
    "\n",
    "tasks = [asyncio.create_task(afetch_doc(i)) for i in range(5)]\n",
    "results = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d853032",
   "metadata": {},
   "source": [
    "The traceback is much longer even for just a simpler function! Imagine a very complicated function with many different await calls!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0ebe52",
   "metadata": {},
   "source": [
    "##### Why do async functions have complex tracebacks then?\n",
    "Because multiple functions are running concurrently. The traceback *splits* because the event loop resumes tasks at different points!\n",
    "> The event loop is like a \"traffic controller\". It manages all async tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96cf60b",
   "metadata": {},
   "source": [
    "#### 2. Coding overhead\n",
    "You need to remember to await your async functions and you must have a very clear mental model of what is happening because things get really messy really quickly with concurrency. Consider the following code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49c53920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 1 exists. Content: Lorem ipsum...\n"
     ]
    }
   ],
   "source": [
    "def check_if_doc_exists(doc_name: str) -> bool:\n",
    "    time.sleep(5) # simulating a DB call\n",
    "    return doc_name in docs\n",
    "\n",
    "def read_document(doc_name: str) -> str:\n",
    "    if check_if_doc_exists(doc_name):\n",
    "        time.sleep(5)\n",
    "        return f\"{doc_name} exists. Content: Lorem ipsum...\"\n",
    "\n",
    "print(read_document(\"doc 1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10d650a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 1 exists. Content: Lorem ipsum...\n"
     ]
    }
   ],
   "source": [
    "async def acheck_if_doc_exists(doc_name: str) -> bool:\n",
    "    await asyncio.sleep(5) # simulating a DB call\n",
    "    return doc_name in docs\n",
    "\n",
    "async def aread_document(doc_name: str) -> str:\n",
    "    if await acheck_if_doc_exists(doc_name): # notice the await here\n",
    "        await asyncio.sleep(5)\n",
    "        return f\"{doc_name} exists. Content: Lorem ipsum...\"\n",
    "\n",
    "print(await aread_document(\"doc 1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7c4ec0",
   "metadata": {},
   "source": [
    "By the way...why \"await\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "895bd579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<coroutine object aread_document at 0x119720700>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zb/r15p7t_d62d8m2s0623s22gh0000gn/T/ipykernel_17355/1783944756.py:1: RuntimeWarning: coroutine 'aread_document' was never awaited\n",
      "  print(aread_document(\"doc 1\"))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "print(aread_document(\"doc 1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf01aab",
   "metadata": {},
   "source": [
    "The object returned from an asynchronous function is known as a coroutine - a task in progress. This task only resolves once you \"await\" it. i.e. adding \"await\" means we wait till the task is complete and thus get the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfaac5b",
   "metadata": {},
   "source": [
    "## 2. The difference between threading and concurrency\n",
    "So far we've made everything work concurrently. But this is actually all happening within the same thread! Simply put, a thread in computer science is exactly like it sounds in social media. \n",
    "\n",
    "On Twitter, you have a thread that talks about subject A and another about B. In computer science, you have a thread working on subject A and can have another thread working on any other subject (including subject A as well). Imagine having multiple threads that are working on concurrent tasks!\n",
    "\n",
    "And Python has a library to do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28f07999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "328c9fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79782acbf1d946c59f27080e0236e368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Worker 0 running on thread ID: 6262042624\n",
      "Worker 1 running on thread ID: 6278868992\n",
      "\n",
      "\n",
      "Worker 2 running on thread ID: 6295695360\n",
      "\n",
      "Worker 3 running on thread ID: 13035925504\n",
      "\n",
      "Worker 4 running on thread ID: 13052751872\n",
      "\n",
      "Worker 0 finished on thread ID: 6262042624\n",
      "Worker 2 finished on thread ID: 6295695360\n",
      "\n",
      "Worker 1 finished on thread ID: 6278868992\n",
      "\n",
      "Worker 3 finished on thread ID: 13035925504\n",
      "\n",
      "Worker 4 finished on thread ID: 13052751872\n",
      "\n",
      "Total time taken: 5.010778903961182 seconds\n"
     ]
    }
   ],
   "source": [
    "def process_info_in_threads(document_name: str, thread_name: str):\n",
    "    thread_id = threading.current_thread().ident\n",
    "    print(f\"\\n{thread_name} running on thread ID: {thread_id}\")\n",
    "    ## Process document ##\n",
    "    result = process_document(document_name)\n",
    "    print(f\"\\n{thread_name} finished on thread ID: {thread_id}\")\n",
    "    return result\n",
    "    \n",
    "start_time = time.time()\n",
    "\n",
    "threads = []\n",
    "for i, doc in tqdm(enumerate(doc)):\n",
    "    working_thread = threading.Thread(target=process_info_in_threads, args=(doc,f\"Worker {i}\",))\n",
    "    threads.append(working_thread)\n",
    "    working_thread.start()\n",
    "\n",
    "# Wait for all threads\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f932daf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Thread(Thread-5 (process_info_in_threads), stopped 6262042624)>,\n",
       " <Thread(Thread-6 (process_info_in_threads), stopped 6278868992)>,\n",
       " <Thread(Thread-7 (process_info_in_threads), stopped 6295695360)>,\n",
       " <Thread(Thread-8 (process_info_in_threads), stopped 13035925504)>,\n",
       " <Thread(Thread-9 (process_info_in_threads), stopped 13052751872)>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116b5a5c",
   "metadata": {},
   "source": [
    "But this reads exactly like the asyncio code doesn't it? The timing is essentially the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52e6a8e",
   "metadata": {},
   "source": [
    "##### Now let's see something dangerous\n",
    "Let's run the exact same block of code again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "808f21f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06fd9b1e71864e91abd463194c0f11ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Worker 0 running on thread ID: 6262042624\n",
      "\n",
      "Worker 0 finished on thread ID: 6262042624\n",
      "Total time taken: 5.0101158618927 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "threads = []\n",
    "for i, doc in tqdm(enumerate(doc)):\n",
    "    working_thread = threading.Thread(target=process_info_in_threads, args=(doc,f\"Worker {i}\",))\n",
    "    threads.append(working_thread)\n",
    "    working_thread.start()\n",
    "\n",
    "# Wait for all threads\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae41dd84",
   "metadata": {},
   "source": [
    "Wait what...why is there just 1 worker now?!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daefb05b",
   "metadata": {},
   "source": [
    "This is known as a *race condition*. `print()` is not thread-safe in python. This means that when multiple threads try to print at the same time their output could get lost!\n",
    "\n",
    "Let's fix it properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebd2f8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_lock = threading.Lock()\n",
    "\n",
    "def fix_process_info_in_threads(document_name: str, thread_name: str):\n",
    "    thread_id = threading.current_thread().ident\n",
    "    \n",
    "    # Thread-safe printing\n",
    "    with print_lock:\n",
    "        print(f\"{thread_name} running on thread ID: {thread_id}\")\n",
    "    \n",
    "    # Process document\n",
    "    result = process_document(document_name)\n",
    "    \n",
    "    # Thread-safe printing\n",
    "    with print_lock:\n",
    "        print(f\"{thread_name} finished on thread ID: {thread_id}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2f1d096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 0 running on thread ID: 6262042624\n",
      "Worker 1 running on thread ID: 6278868992\n",
      "Worker 2 running on thread ID: 6295695360\n",
      "Worker 3 running on thread ID: 13035925504\n",
      "Worker 4 running on thread ID: 13052751872\n",
      "Worker 0 finished on thread ID: 6262042624\n",
      "Worker 1 finished on thread ID: 6278868992\n",
      "Worker 2 finished on thread ID: 6295695360\n",
      "Worker 3 finished on thread ID: 13035925504\n",
      "Worker 4 finished on thread ID: 13052751872\n",
      "Total time taken: 5.01 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "threads = []\n",
    "for i, doc in enumerate(docs):  # Fixed: was tqdm(enumerate(doc))\n",
    "    working_thread = threading.Thread(\n",
    "        target=fix_process_info_in_threads, \n",
    "        args=(doc, f\"Worker {i}\")\n",
    "    )\n",
    "    threads.append(working_thread)\n",
    "    working_thread.start()\n",
    "\n",
    "# Wait for all threads\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111431a7",
   "metadata": {},
   "source": [
    "Race conditions happen in `asyncio` too! Tasks \"race\" each other to the finish line. So the asyncio has its own `lock` capability as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a03a647",
   "metadata": {},
   "source": [
    "## 3. The difference between concurrency, threading and parallelism\n",
    "\n",
    "I hate to burst your bubble, but Python's threads are not truly parallel because threads don't run at the exact same time - they're just *started at the same time*. This is by design. Python's Global Interpreter Lock (GIL) means that the Python interpreter is only accessible by a single thread at any point of time!\n",
    "\n",
    "### The Global Interpreter Lock (GIL)\n",
    "Python's GIL is a mutex that prevents multiple native threads from executing Python bytecode simultaneously. This means:\n",
    "\n",
    "- Only one thread can execute Python code at a time\n",
    "- Threads take turns getting access to the Python interpreter\n",
    "- For CPU-intensive tasks, threading often provides no performance benefit and can even be slower due to context switching overhead\n",
    "\n",
    "> When you're waiting for something external (network, disk, database), your CPU is just sitting idle. Asyncio lets your single thread work on other tasks while waiting.\n",
    "\n",
    "So how do we get true parallelism where everything is happening at the same time? Use a `ProcessPoolExecutor`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa73050",
   "metadata": {},
   "source": [
    "### Where Threading and Multiprocessing Fit In\n",
    "1. asyncio (Single Thread)\n",
    "\n",
    "- Best for: I/O-bound tasks (web requests, file operations, database calls)\n",
    "- How it works: Cooperative multitasking - tasks voluntarily yield control\n",
    "- GIL impact: No impact because it's single-threaded anyway\n",
    "\n",
    "2. threading (Multiple Threads, GIL Limited)\n",
    "\n",
    "- Best for: I/O-bound tasks when you can't use asyncio\n",
    "- How it works: Preemptive multitasking - OS switches between threads\n",
    "- GIL impact: Only one thread executes Python code at a time, but threads release GIL during I/O waits\n",
    "\n",
    "3. multiprocessing (Multiple Processes)\n",
    "\n",
    "- Best for: CPU-bound tasks (heavy calculations, data processing)\n",
    "- How it works: Separate Python interpreters, each with its own GIL\n",
    "- GIL impact: None - each process has its own GIL\n",
    "\n",
    "> In general, use `asyncio` as much as you can instead of `threading` because Python's GIL means that they work the same - unless the framework does not have an async/await methods (i.e fully synchronous libraries like numpy, pandas, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8894834",
   "metadata": {},
   "source": [
    "But there's a slight problem with parallelism in Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "233b3e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThreadPoolExecutor (CPU): 47.65s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnProcess-1:\n",
      "Traceback (most recent call last):\n",
      "Process SpawnProcess-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py313/lib/python3.13/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/opt/anaconda3/envs/py313/lib/python3.13/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/py313/lib/python3.13/concurrent/futures/process.py\", line 242, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/anaconda3/envs/py313/lib/python3.13/multiprocessing/queues.py\", line 120, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "AttributeError: Can't get attribute 'cpu_intensive_work' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnProcess-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py313/lib/python3.13/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/opt/anaconda3/envs/py313/lib/python3.13/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/py313/lib/python3.13/concurrent/futures/process.py\", line 242, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/anaconda3/envs/py313/lib/python3.13/multiprocessing/queues.py\", line 120, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "AttributeError: Can't get attribute 'cpu_intensive_work' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/opt/anaconda3/envs/py313/lib/python3.13/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/opt/anaconda3/envs/py313/lib/python3.13/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/py313/lib/python3.13/concurrent/futures/process.py\", line 242, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/anaconda3/envs/py313/lib/python3.13/multiprocessing/queues.py\", line 120, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "AttributeError: Can't get attribute 'cpu_intensive_work' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnProcess-4:\n",
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBrokenProcessPool\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m start = time.time()  \n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ProcessPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     results = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcpu_intensive_work\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpu_tasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessPoolExecutor (CPU): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# ~2s (4x speedup!)\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# I/O-bound work comparison:\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/py313/lib/python3.13/concurrent/futures/process.py:617\u001b[39m, in \u001b[36m_chain_from_iterable_of_lists\u001b[39m\u001b[34m(iterable)\u001b[39m\n\u001b[32m    611\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[32m    612\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    613\u001b[39m \u001b[33;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[32m    614\u001b[39m \u001b[33;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[32m    615\u001b[39m \u001b[33;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[32m    616\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m617\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m        \u001b[49m\u001b[43melement\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwhile\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/py313/lib/python3.13/concurrent/futures/_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/py313/lib/python3.13/concurrent/futures/_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/py313/lib/python3.13/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/py313/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mBrokenProcessPool\u001b[39m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "\n",
    "def cpu_intensive_work(n):\n",
    "    \"\"\"Heavy computation\"\"\"\n",
    "    total = 0\n",
    "    for i in range(n * 1_000_000):\n",
    "        total += i ** 2\n",
    "    return total\n",
    "\n",
    "def io_intensive_work(delay):\n",
    "    \"\"\"I/O simulation\"\"\"\n",
    "    time.sleep(delay)\n",
    "    return f\"Slept for {delay} seconds\"\n",
    "\n",
    "# Test data\n",
    "cpu_tasks = [100, 200, 300, 400]\n",
    "io_tasks = [1, 1, 1, 1]\n",
    "\n",
    "# CPU-bound work comparison:\n",
    "start = time.time()\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = list(executor.map(cpu_intensive_work, cpu_tasks))\n",
    "print(f\"ThreadPoolExecutor (CPU): {time.time() - start:.2f}s\")  # ~8s (no speedup)\n",
    "\n",
    "start = time.time()  \n",
    "with ProcessPoolExecutor() as executor:\n",
    "    results = list(executor.map(cpu_intensive_work, cpu_tasks))\n",
    "print(f\"ProcessPoolExecutor (CPU): {time.time() - start:.2f}s\")  # ~2s (4x speedup!)\n",
    "\n",
    "# I/O-bound work comparison:\n",
    "start = time.time()\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = list(executor.map(io_intensive_work, io_tasks))\n",
    "print(f\"ThreadPoolExecutor (I/O): {time.time() - start:.2f}s\")  # ~1s (good speedup)\n",
    "\n",
    "start = time.time()\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    results = list(executor.map(io_intensive_work, io_tasks))\n",
    "print(f\"ProcessPoolExecutor (I/O): {time.time() - start:.2f}s\")  # ~1s (same, but more overhead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d83d67",
   "metadata": {},
   "source": [
    "Multiprocessing needs to pickle (serialize) your function to send it to other processes. Jupyter notebooks has issues with this:\n",
    "\n",
    "- Jupyter thinks your function is in \"__main__\" \n",
    "- But new processes can't find \"__main__\" from a notebook\n",
    "```\n",
    "def cpu_intensive_work(n):  # This can't be pickled properly\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abffd3c4",
   "metadata": {},
   "source": [
    "So let's just save the functions in a .py file and import them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3fce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "from worker_functions import cpu_intensive_work, io_intensive_work\n",
    "\n",
    "# --------------------------\n",
    "# CPU-bound benchmarks\n",
    "# --------------------------\n",
    "\n",
    "def test_threadpool_cpu():\n",
    "    cpu_tasks = [100, 200, 300, 400]\n",
    "    start = time.time()\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(cpu_intensive_work, cpu_tasks))\n",
    "    print(f\"ThreadPoolExecutor (CPU): {time.time() - start:.2f}s\")\n",
    "\n",
    "\n",
    "def test_processpool_cpu():\n",
    "    cpu_tasks = [100, 200, 300, 400]\n",
    "    start = time.time()\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        results = list(executor.map(cpu_intensive_work, cpu_tasks))\n",
    "    print(f\"ProcessPoolExecutor (CPU): {time.time() - start:.2f}s\")\n",
    "\n",
    "\n",
    "async def async_cpu_test():\n",
    "    cpu_tasks = [100, 200, 300, 400]\n",
    "    loop = asyncio.get_running_loop()\n",
    "    start = time.time()\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        results = await asyncio.gather(*[\n",
    "            loop.run_in_executor(executor, cpu_intensive_work, task)\n",
    "            for task in cpu_tasks\n",
    "        ])\n",
    "    print(f\"Asyncio + ProcessPool (CPU): {time.time() - start:.2f}s\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# I/O-bound benchmarks\n",
    "# --------------------------\n",
    "\n",
    "def test_threadpool_io(n_tasks=10, delay=1, max_workers=10):\n",
    "    start = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        results = list(executor.map(io_intensive_work, [delay] * n_tasks))\n",
    "    print(f\"ThreadPool (I/O, {n_tasks} tasks, {max_workers} workers): {time.time() - start:.2f}s\")\n",
    "\n",
    "async def test_asyncio_io(n_tasks=10, delay=1):\n",
    "    start = time.time()\n",
    "    results = await asyncio.gather(*[asyncio.sleep(delay) for _ in range(n_tasks)])\n",
    "    print(f\"Asyncio (I/O, {n_tasks} tasks): {time.time() - start:.2f}s\")\n",
    "\n",
    "def test_processpool_io(n_tasks=10, delay=1, max_workers=10):\n",
    "    start = time.time()\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        results = list(executor.map(io_intensive_work, [delay] * n_tasks))\n",
    "    print(f\"ProcessPool (I/O, {n_tasks} tasks, {max_workers} workers): {time.time() - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4dfd57a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CPU-bound ===\n",
      "ThreadPoolExecutor (CPU): 46.35s\n",
      "ProcessPoolExecutor (CPU): 20.14s\n",
      "Asyncio + ProcessPool (CPU): 19.89s\n"
     ]
    }
   ],
   "source": [
    "print(\"=== CPU-bound ===\")\n",
    "test_threadpool_cpu()\n",
    "test_processpool_cpu()\n",
    "await async_cpu_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5677ad90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== I/O-bound ===\n",
      "ThreadPool (I/O, 10 tasks, 5 workers): 2.01s\n",
      "ThreadPool (I/O, 10 tasks, 20 workers): 1.01s\n",
      "Asyncio (I/O, 10 tasks): 1.00s\n",
      "ProcessPool (I/O, 10 tasks, 20 workers): 1.10s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== I/O-bound ===\")\n",
    "test_threadpool_io(n_tasks=10, delay=1, max_workers=5)\n",
    "test_threadpool_io(n_tasks=10, delay=1, max_workers=20)\n",
    "await test_asyncio_io(n_tasks=10, delay=1)\n",
    "test_processpool_io(n_tasks=10, delay=1, max_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863caa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== I/O-bound (scaled) ===\n",
      "ThreadPool (I/O, 100 tasks, 10 workers): 10.04s\n",
      "ThreadPool (I/O, 100 tasks, 100 workers): 1.02s\n",
      "ProcessPool (I/O, 100 tasks, 100 workers): 1.45s\n",
      "Asyncio (I/O, 100 tasks): 1.00s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== I/O-bound (scaled) ===\")\n",
    "test_threadpool_io(n_tasks=100, delay=1, max_workers=10)\n",
    "test_threadpool_io(n_tasks=100, delay=1, max_workers=100)\n",
    "test_processpool_io(n_tasks=100, delay=1, max_workers=100)\n",
    "await test_asyncio_io(n_tasks=100, delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378844d3",
   "metadata": {},
   "source": [
    "# TLDR\n",
    "\n",
    "##### I/O-bound tasks\n",
    "- It doesn’t matter whether you use threads, processes, or asyncio — performance is basically the same.\n",
    "- The “win” comes from concurrency: letting the program do something else while waiting on I/O.\n",
    "- That’s why ThreadPool, asyncio, and asyncio+ThreadPool all landed around ~1 second.\n",
    "\n",
    "##### CPU-bound tasks\n",
    "- Threads don’t help, because of the Global Interpreter Lock (GIL) — they still run one at a time, so performance was ~45s.\n",
    "- Processes (ProcessPoolExecutor or asyncio+ProcessPool) bypass the GIL by running in separate Python interpreters, so they run in parallel across CPU cores, giving you ~19–20s.\n",
    "- Asyncio itself doesn’t provide speedup here, it just gives you nicer orchestration around using a ProcessPool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37808a92",
   "metadata": {},
   "source": [
    "## An analogy\n",
    "\n",
    "### Asynchronous\n",
    "- Definition: A programming style where tasks can be paused and resumed, so one task doesn’t block others from starting.\n",
    "- Analogy: Ordering food at a restaurant without waiting for the guy in front of you to get his food. \n",
    "\n",
    "### Concurrency\n",
    "- Definition: The ability to deal with many tasks at once by interleaving progress. Doesn’t necessarily mean they’re literally executing at the same instant.\n",
    "- Analogy: I’m breathing, blinking, thinking and coding at work. But I’m still one dude.\n",
    "\n",
    "### Parallelism\n",
    "- Definition: Tasks literally run at the same time, usually on multiple CPU cores or machines.\n",
    "- Analogy: My team and I are working at the same time (during office hours) and are building different features to the same app."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parallelism",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
